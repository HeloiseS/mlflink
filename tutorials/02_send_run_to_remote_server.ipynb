{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial 2: Sending Selected Runs to Remote Server\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "- Understand how to identify your best local runs\n",
        "- Learn to retrieve artifacts from local MLflow runs\n",
        "- Master the process of re-running experiments on a remote server\n",
        "- Avoid overloading remote servers with unnecessary data\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "- Completed **Tutorial 1** (local MLflow setup)\n",
        "- At least one successful run in your local MLflow server\n",
        "- Access credentials to the remote MLflow server (username + password)\n",
        "- Understanding of the `mlruns/` and `mlartifacts/` directory structure\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Key Concept: Why Re-run Instead of Transfer?\n",
        "\n",
        "MLflow doesn't have a \"copy run\" feature. Instead, we:\n",
        "\n",
        "1. **Experiment freely locally** (unlimited runs, no server load)\n",
        "2. **Identify the best model** (using local UI/metrics)\n",
        "3. **Re-run ONLY that model** on the remote server (with same params, data, model)\n",
        "\n",
        "This approach:\n",
        "-  Keeps remote servers clean and organized\n",
        "-  Reduces bandwidth and storage costs\n",
        "-  Ensures reproducibility\n",
        "-  Maintains complete experiment history locally\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setting Up Remote Server Credentials\n",
        "\n",
        "Before connecting to the remote server, you need to set up authentication.\n",
        "\n",
        "### üîê Set environment variables\n",
        "\n",
        "In your terminal (or add to your `.bashrc`/`.zshrc`):\n",
        "\n",
        "```bash\n",
        "export MLFLOW_TRACKING_USERNAME=\"your_username\"\n",
        "export MLFLOW_TRACKING_PASSWORD=\"your_password\"\n",
        "export MLFLOW_TRACKING_URI=\"https://mlflow-dev.fink-broker.org\"\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è **Security Note**: Never hardcode credentials in your notebooks or scripts!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify environment variables are set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if credentials are set\n",
        "required_vars = [\"MLFLOW_TRACKING_USERNAME\", \"MLFLOW_TRACKING_PASSWORD\", \"MLFLOW_TRACKING_URI\"]\n",
        "\n",
        "for var in required_vars:\n",
        "    if var in os.environ:\n",
        "        print(f\"{var} is set\")\n",
        "    else:\n",
        "        print(f\"{var} is NOT set - please set it before continuing!\")\n",
        "        \n",
        "# Display the tracking URI (safe to show)\n",
        "if \"MLFLOW_TRACKING_URI\" in os.environ:\n",
        "    print(f\"\\nüîó Remote server: {os.environ['MLFLOW_TRACKING_URI']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Identifying Your Best Local Run\n",
        "\n",
        "First, let's look at your local runs to identify which one to send to the remote server.\n",
        "\n",
        "### üìÅ Understanding the Local Directory Structure\n",
        "\n",
        "Navigate to where you ran `mlflow server` in Tutorial 1. You should see:\n",
        "\n",
        "```\n",
        "your_directory/\n",
        "‚îú‚îÄ‚îÄ mlruns/              # Metadata\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ <experiment_id>/\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ <run_id>/\n",
        "‚îî‚îÄ‚îÄ mlartifacts/         # Actual artifacts\n",
        "    ‚îî‚îÄ‚îÄ <experiment_id>/\n",
        "        ‚îî‚îÄ‚îÄ <run_id>/\n",
        "```\n",
        "\n",
        "### üì∏ Screenshot placeholder: Directory structure\n",
        "‚ö†Ô∏è **NOTE: The long strings of numbers and letters (experiment IDs and run IDs) will be DIFFERENT for you!**\n",
        "extract from tree \n",
        "\n",
        "```\n",
        "mlruns/\n",
        "‚îú‚îÄ‚îÄ 0 # default experiemnt\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ meta.yaml\n",
        "‚îú‚îÄ‚îÄ 256835725489686937 # tutorial experiement\n",
        "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 159b8d88e56446dfb595b540960b99be # run\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ artifacts\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ meta.yaml\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ metrics\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ accuracy\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ f1_score\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ m-34aaeb7bdad14f93a3e76ed769f7bf18\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ meta.yaml\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ params\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ learning_rate\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ random_state\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tags\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ mlflow.runName\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ mlflow.source.name\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ mlflow.source.type\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ mlflow.user\n",
        "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ meta.yaml\n",
        "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models # model use for this run \n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ m-34aaeb7bdad14f93a3e76ed769f7bf18\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ meta.yaml\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ metrics\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ accuracy\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ f1_score\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ params\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ learning_rate\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ random_state\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tags\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ mlflow.source.name\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ mlflow.source.type\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ mlflow.user\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tags\n",
        "‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ mlflow.experimentKind\n",
        "‚îî‚îÄ‚îÄ models\n",
        "mlartifacts/\n",
        "‚îî‚îÄ‚îÄ 256835725489686937\n",
        "    ‚îú‚îÄ‚îÄ d62f60d9ecde424ea0bdc30352bf7143\n",
        "    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ artifacts\n",
        "    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ meta.json\n",
        "    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ X_train.parquet\n",
        "    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ y_train.parquet\n",
        "    ‚îî‚îÄ‚îÄ models\n",
        "        ‚îú‚îÄ‚îÄ m-34aaeb7bdad14f93a3e76ed769f7bf18\n",
        "        ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ artifacts\n",
        "        ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ conda.yaml\n",
        "        ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ input_example.json\n",
        "        ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ MLmodel\n",
        "        ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ model.pkl\n",
        "        ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ python_env.yaml\n",
        "        ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ requirements.txt\n",
        "        ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ serving_input_example.json\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Find your experiment and run IDs\n",
        "\n",
        "You can find these either:\n",
        "1. **In the MLflow UI** (URL shows experiment and run IDs)\n",
        "2. **Programmatically** (as we did in Tutorial 1)\n",
        "3. **By browsing the file system**\n",
        "\n",
        "Let's do it programmatically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to LOCAL MLflow server\n",
        "LOCAL_HOST = \"http://127.0.0.1\"\n",
        "LOCAL_PORT = 6969\n",
        "mlflow.set_tracking_uri(f\"{LOCAL_HOST}:{LOCAL_PORT}\")\n",
        "\n",
        "client = MlflowClient()\n",
        "\n",
        "# Get the experiment\n",
        "EXPERIMENT_NAME = \"tutorial\"  # From Tutorial 1\n",
        "experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
        "\n",
        "if experiment is None:\n",
        "    print(f\"Experiment '{EXPERIMENT_NAME}' not found!\")\n",
        "    print(\"Make sure you completed Tutorial 1 first.\")\n",
        "else:\n",
        "    print(f\"Found experiment: {EXPERIMENT_NAME}\")\n",
        "    print(f\"Experiment ID: {experiment.experiment_id}\")\n",
        "    \n",
        "    # Get all runs\n",
        "    runs = client.search_runs(experiment.experiment_id)\n",
        "    print(f\"Total runs: {len(runs)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View all runs and select the best one\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all runs with their metrics\n",
        "run_data = []\n",
        "for run in runs:\n",
        "    run_data.append({\n",
        "        \"Run Name\": run.data.tags.get(\"mlflow.runName\", \"N/A\"),\n",
        "        \"Run ID\": run.info.run_id,\n",
        "        \"Learning Rate\": run.data.params.get(\"learning_rate\", \"N/A\"),\n",
        "        \"Accuracy\": run.data.metrics.get(\"accuracy\", \"N/A\"),\n",
        "        \"F1 Score\": run.data.metrics.get(\"f1_score\", \"N/A\"),\n",
        "        \"Status\": run.info.status\n",
        "    })\n",
        "\n",
        "runs_df = pd.DataFrame(run_data)\n",
        "print(\"Available runs:\")\n",
        "display(runs_df.sort_values(\"Accuracy\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Select the run you want to send\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the best run (highest accuracy in this case)\n",
        "# In practice, you might choose based on different criteria\n",
        "best_run = runs[0]  # Assuming sorted by best metric\n",
        "for run in runs:\n",
        "    acc = run.data.metrics.get(\"accuracy\", 0)\n",
        "    if acc >= best_run.data.metrics.get(\"accuracy\", 0):\n",
        "        best_run = run\n",
        "\n",
        "print(\"Selected run to send:\")\n",
        "print(f\"  - Name: {best_run.data.tags.get('mlflow.runName', 'N/A')}\")\n",
        "print(f\"  - Run ID: {best_run.info.run_id}\")\n",
        "print(f\"  - Accuracy: {best_run.data.metrics.get('accuracy', 'N/A')}\")\n",
        "print(f\"  - F1 Score: {best_run.data.metrics.get('f1_score', 'N/A')}\")\n",
        "\n",
        "# Store IDs for later use\n",
        "SELECTED_RUN_ID = best_run.info.run_id\n",
        "EXPERIMENT_ID = experiment.experiment_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Retrieving Artifacts from the Local Run\n",
        "\n",
        "Now we need to gather all the information from this run to reproduce it on the remote server.\n",
        "\n",
        "### üóÇÔ∏è Configure paths\n",
        "\n",
        "‚ö†Ô∏è **Important**: Update `root_dir` to where YOUR `mlflow server` was running!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# UPDATE THIS PATH to where you ran `mlflow server`\n",
        "# This directory should contain mlruns/ and mlartifacts/\n",
        "root_dir = Path().resolve().parent # Assumes parent directory, adjust if needed\n",
        "\n",
        "print(f\"Root directory: {root_dir}\")\n",
        "print(f\"   Looking for mlruns/ and mlartifacts/...\")\n",
        "\n",
        "# Verify directories exist\n",
        "mlruns_dir = root_dir / \"mlruns\"\n",
        "mlartifacts_dir = root_dir / \"mlartifacts\"\n",
        "\n",
        "if mlruns_dir.exists():\n",
        "    print(f\"   Found mlruns/\")\n",
        "else:\n",
        "    print(f\"    mlruns/ not found! Update root_dir\")\n",
        "    \n",
        "if mlartifacts_dir.exists():\n",
        "    print(f\"    Found mlartifacts/\")\n",
        "else:\n",
        "    print(f\"    mlartifacts/ not found! Update root_dir\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üì¶ Load training data from artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build paths to the artifacts\n",
        "artifacts_path = mlartifacts_dir / EXPERIMENT_ID / SELECTED_RUN_ID / \"artifacts\"\n",
        "\n",
        "path_X = artifacts_path / \"X_train.parquet\"\n",
        "path_y = artifacts_path / \"y_train.parquet\"\n",
        "\n",
        "print(f\"Artifacts path: {artifacts_path}\")\n",
        "\n",
        "# Load the data\n",
        "if path_X.exists() and path_y.exists():\n",
        "    X = pd.read_parquet(path_X)\n",
        "    y = pd.read_parquet(path_y)\n",
        "    print(f\" Loaded training data:\")\n",
        "    print(f\"   - X shape: {X.shape}\")\n",
        "    print(f\"   - y shape: {y.shape}\")\n",
        "else:\n",
        "    print(\" Training data not found in artifacts!\")\n",
        "    print(\"   Make sure you logged the data in Tutorial 1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è Load model parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metadata (contains parameters)\n",
        "meta_path = artifacts_path / \"meta.json\"\n",
        "\n",
        "if meta_path.exists():\n",
        "    with open(meta_path, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "    PARAMS = metadata['params']\n",
        "    print(f\" Loaded parameters: {PARAMS}\")\n",
        "else:\n",
        "    print(\" meta.json not found!\")\n",
        "    print(\"   Falling back to run parameters...\")\n",
        "    PARAMS = best_run.data.params\n",
        "    print(f\"   Parameters: {PARAMS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Environment Dependencies\n",
        "\n",
        "The remote server needs to know which packages are required to run your preprocessing code.\n",
        "\n",
        "This project includes a utility to automatically extract dependencies from your code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.abspath(\"..\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflink.utils.env_utils import generate_requirements_txt_from_imports\n",
        "from importlib import resources\n",
        "\n",
        "# Get path to preprocessing module\n",
        "with resources.path(\"mlflink\", \"processing\") as module_path:\n",
        "    PKG_DIR = module_path\n",
        "\n",
        "print(f\" Analyzing dependencies in: {PKG_DIR}\")\n",
        "\n",
        "# Generate requirements.txt\n",
        "dependencies_path = generate_requirements_txt_from_imports(\n",
        "    PKG_DIR, \n",
        "    \"/tmp/requirements.txt\", \n",
        "    include_self=False\n",
        ")\n",
        "\n",
        "print(f\" Generated requirements.txt at: {dependencies_path}\")\n",
        "\n",
        "# Display the generated requirements\n",
        "print(\"\\n Dependencies to be sent to remote server:\")\n",
        "with open(dependencies_path, \"r\") as f:\n",
        "    print(f.read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Why This Matters\n",
        "\n",
        "The remote server (Fink) needs:\n",
        "1. Your trained model (for making predictions)\n",
        "2. Your preprocessing code (to prepare new data)\n",
        "3. Environment dependencies (to run your code)\n",
        "\n",
        "MLflow automatically handles (1), but we need to explicitly log (2) and (3).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Re-run on Remote Server\n",
        "\n",
        "Now we have everything we need! Let's re-run the experiment on the remote server.\n",
        "\n",
        "### üåê Switch to remote tracking URI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from mlflow.models import infer_signature\n",
        "import numpy as np\n",
        "\n",
        "# Get remote server URL from environment\n",
        "# REMOTE_URI = os.environ.get(\"MLFLOW_TRACKING_URI\", \"https://mlflow-dev.fink-broker.org\")\n",
        "REMOTE_URI = \"https://mlflow-dev.fink-broker.org\"\n",
        "\n",
        "print(f\" Switching from local to remote server...\")\n",
        "print(f\"   Local:  {LOCAL_HOST}:{LOCAL_PORT}\")\n",
        "print(f\"   Remote: {REMOTE_URI}\")\n",
        "\n",
        "# Switch to remote server\n",
        "mlflow.set_tracking_uri(REMOTE_URI)\n",
        "mlflow.set_experiment(\"tutorial\")  # Use same experiment name\n",
        "\n",
        "client = MlflowClient()\n",
        "\n",
        "print(f\" Connected to remote server!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üöÄ Run the experiment on remote server\n",
        "\n",
        "This is almost identical to Tutorial 1, but with TWO critical additions for the remote server:\n",
        "1. Log the preprocessing code\n",
        "2. Log the requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting run on REMOTE server...\\\\n\")\n",
        "\n",
        "with mlflow.start_run(run_name=f\"remote_LR_{PARAMS['learning_rate']}\"):\n",
        "    \n",
        "    # ==========================================\n",
        "    # 1. TRAIN MODEL (same as before)\n",
        "    # ==========================================\n",
        "    print(\"  Training model...\")\n",
        "    model = HistGradientBoostingClassifier(**PARAMS)\n",
        "    model.fit(X.values, y)\n",
        "    y_pred = model.predict(X.values)\n",
        "    print(\" Model trained!\\\\n\")\n",
        "    \n",
        "    # ==========================================\n",
        "    # 2. LOG PARAMETERS (same as before)\n",
        "    # ==========================================\n",
        "    print(\" Logging parameters...\")\n",
        "    mlflow.log_params(PARAMS)\n",
        "    \n",
        "    # ==========================================\n",
        "    # 3. LOG MODEL (same as before)\n",
        "    # ==========================================\n",
        "    print(\" Logging model...\")\n",
        "    signature = infer_signature(X, y_pred)\n",
        "    mlflow.sklearn.log_model(\n",
        "        model,\n",
        "        artifact_path=\"model\",\n",
        "        signature=signature,\n",
        "        input_example=X.iloc[:1],\n",
        "    )\n",
        "    \n",
        "    # ==========================================\n",
        "    # 4. LOG METRICS (same as before)\n",
        "    # ==========================================\n",
        "    print(\" Logging metrics...\")\n",
        "    mlflow.log_metric(\"accuracy\", accuracy_score(y, y_pred))\n",
        "    mlflow.log_metric(\"precision\", precision_score(y, y_pred, zero_division=0))\n",
        "    mlflow.log_metric(\"recall\", recall_score(y, y_pred, zero_division=0))\n",
        "    mlflow.log_metric(\"f1_score\", f1_score(y, y_pred, zero_division=0))\n",
        "    \n",
        "    # ==========================================\n",
        "    # 5. LOG DATA (optional - be selective!)\n",
        "    # ==========================================\n",
        "    print(\" Logging training data...\")\n",
        "    mlflow.log_table(X, \"X_train.parquet\")\n",
        "    mlflow.log_table(y, \"y_train.parquet\")\n",
        "    \n",
        "    # ==========================================\n",
        "    # 6. LOG METADATA (same as before)\n",
        "    # ==========================================\n",
        "    print(\" Logging metadata...\")\n",
        "    meta_info = {\n",
        "        \"params\": PARAMS,\n",
        "        \"data_info\": {\n",
        "            \"n_samples\": X.shape[0],\n",
        "            \"n_features\": X.shape[1]\n",
        "        },\n",
        "        \"notes\": \"Run sent from local to remote server\",\n",
        "        \"original_run_id\": SELECTED_RUN_ID\n",
        "    }\n",
        "    with open(\"meta.json\", \"w\") as f:\n",
        "        json.dump(meta_info, f, indent=2)\n",
        "    mlflow.log_artifact(\"meta.json\")\n",
        "    \n",
        "    # ==========================================\n",
        "    # 7. LOG PREPROCESSING CODE NEW FOR REMOTE üÜï\n",
        "    # ==========================================\n",
        "    # This is CRITICAL for deployment! The remote server (Fink) needs your \n",
        "    # preprocessing code to transform new incoming data the same way you \n",
        "    # transformed your training data. Without this, the model won't work!\n",
        "    #\n",
        "    # Why we log the entire processing directory:\n",
        "    # - Other developers can reproduce your exact preprocessing pipeline\n",
        "    # - The Fink server can apply the same transformations to live data\n",
        "    # - Ensures consistency between training and production inference\n",
        "    # - Makes your model deployment fully reproducible\n",
        "    print(\" Logging preprocessing code...\")\n",
        "    with resources.path(\"mlflink\", \"processing\") as preprocessing_path:\n",
        "        mlflow.log_artifacts(str(preprocessing_path), name=\"code\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 8. LOG REQUIREMENTS NEW FOR REMOTE üÜï\n",
        "    # ==========================================\n",
        "    # The requirements.txt file tells the remote server which Python packages\n",
        "    # are needed to run your preprocessing code. Without this, your code \n",
        "    # might fail due to missing dependencies!\n",
        "    #\n",
        "    # Why this matters:\n",
        "    # - Ensures the remote environment has all necessary libraries\n",
        "    # - Other developers know exactly which versions you used\n",
        "    # - Prevents \"works on my machine\" problems\n",
        "    # - Critical for the Fink server to execute your preprocessing pipeline\n",
        "    #\n",
        "    # Note: This only includes dependencies for preprocessing, not the model\n",
        "    # (MLflow handles model dependencies automatically)\n",
        "    print(\" Logging dependencies...\")\n",
        "    mlflow.log_artifact(dependencies_path)\n",
        "    \n",
        "    print(\"\\\\n Run completed successfully on REMOTE server!\")\n",
        "    print(f\" View at: {REMOTE_URI}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Verify the Remote Run\n",
        "\n",
        "Let's verify that everything was uploaded correctly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query the remote server\n",
        "remote_experiment = client.get_experiment_by_name(\"tutorial\")\n",
        "remote_runs = client.search_runs(remote_experiment.experiment_id, max_results=5)\n",
        "\n",
        "print(f\"üìä Latest runs on remote server:\\\\n\")\n",
        "\n",
        "for i, run in enumerate(remote_runs[:3], 1):\n",
        "    print(f\"{i}. {run.data.tags.get('mlflow.runName', 'N/A')}\")\n",
        "    print(f\"   - Run ID: {run.info.run_id[:8]}...\")\n",
        "    print(f\"   - Accuracy: {run.data.metrics.get('accuracy', 'N/A')}\")\n",
        "    print(f\"   - Status: {run.info.status}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. What We Sent vs. What We Didn't\n",
        "\n",
        "### ‚úÖ What we sent to the remote server:\n",
        "- Trained model (sklearn artifact)\n",
        "- Model parameters\n",
        "- Performance metrics\n",
        "- Training data (X and y) - optional\n",
        "- Preprocessing code (entire `processing/` directory)\n",
        "- Requirements.txt (dependencies)\n",
        "- Metadata (custom info)\n",
        "\n",
        "### üôÖüèø What we didn't send:\n",
        "- All the failed/experimental runs\n",
        "- Intermediate debugging runs\n",
        "- Large datasets from experimentation\n",
        "- Temporary files\n",
        "\n",
        "This keeps the remote server clean and focused on production-ready models!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Best Practices for Remote Runs\n",
        "\n",
        "### ‚úÖ DO:\n",
        "- **Experiment extensively locally first** (unlimited runs)\n",
        "- **Send only your best/final models** to remote\n",
        "- **Include preprocessing code** (critical for deployment)\n",
        "- **Log dependencies** (requirements.txt)\n",
        "- **Use descriptive run names** (easier to find later)\n",
        "- **Add metadata** about the original local run\n",
        "\n",
        "### üôÖüèø DON'T:\n",
        "- **Send every experimental run** (overloads server)\n",
        "- **Log huge datasets** unless absolutely necessary\n",
        "- **Forget to include preprocessing code** (model won't work without it)\n",
        "- **Skip the requirements.txt** (environment issues)\n",
        "- **Send runs with errors or incomplete training**\n",
        "\n",
        "### üí° Recommended Workflow:\n",
        "\n",
        "```\n",
        "1. Local: Run 50 experiments with different parameters\n",
        "2. Local: Identify the best 1-3 models\n",
        "3. Local: Do a final run with those params + full data logging\n",
        "4. Remote: Send ONLY those 1-3 final runs\n",
        "5. Remote: Verify they work correctly\n",
        "```\n",
        "\n",
        "This approach:\n",
        "- Maximizes your experimentation freedom locally\n",
        "- Minimizes remote server load\n",
        "- Keeps remote experiments organized and meaningful\n",
        "- Reduces costs (bandwidth, storage)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  <img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExeXd2ZTNscDZtbm1jbDB2dDJveGp2ZWdtMmhsMDg0MmdvdnpncmJzZSZlcD12MV9zdGlja2Vyc19zZWFyY2gmY3Q9cw/xUPGchCSJlq3NzwLyU/giphy.gif\" width=\"50\" height=\"50\"> WHY  Steps 7 & 8 Are Essential for Remote Deployment\n",
        "\n",
        "When you send a model to the Fink server, you're not just sending the trained model weights. \n",
        "For the model to work in production, Fink needs:\n",
        "\n",
        "1. **Your preprocessing code** (Step 7):\n",
        "   - The server receives raw alert data\n",
        "   - It needs to transform this data exactly as you did during training\n",
        "   - Your `make_cut()`, `raw2clean()`, `run_sherlock()`, and `make_X()` functions\n",
        "   - Without this, the model would receive incorrectly formatted data\n",
        "\n",
        "2. **Your dependencies** (Step 8):\n",
        "   - Your preprocessing code might use specific libraries (pandas, numpy, lasair, etc.)\n",
        "   - The server needs to know which versions to install\n",
        "   - Prevents runtime errors due to missing or incompatible packages\n",
        "\n",
        "**Real-world scenario:**\n",
        "- You train a model locally with your preprocessing pipeline\n",
        "- Fink receives new alerts and needs to classify them\n",
        "- Fink loads your model AND your preprocessing code\n",
        "- For each alert: `raw data ‚Üí your preprocessing ‚Üí your model ‚Üí prediction`\n",
        "- Other scientists can also download and run your complete pipeline\n",
        "\n",
        "**Without these steps**, your model would be useless on the remote server! üö´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<iframe src=\"https://giphy.com/embed/S3nZ8V9uemShxiWX8g\" width=\"40\" height=\"40\" style=\"\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>Congratulations! You finished Tutorial 2!<iframe src=\"https://giphy.com/embed/8fftcK2D4PK6XCs2P0\" width=\"60\" height=\"60\" style=\"\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "In **Tutorial 3** ??? (optional), you'll learn how to:\n",
        "<!-- - Customize the preprocessing pipeline for your data\n",
        "- Replace the example model with your own\n",
        "- Adapt this template for your specific project -->\n",
        "\n",
        "---\n",
        "\n",
        "## üÜò Troubleshooting  <iframe src=\"https://giphy.com/embed/PnpkimJ5mrZRe\" width=\"200\" height=\"200\" style=\"\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
        "\n",
        "### Problem: Authentication failed\n",
        "**Solution**: Double-check your environment variables are set correctly. Try logging out and back in.\n",
        "\n",
        "### Problem: Can't find local artifacts\n",
        "**Solution**: Make sure `root_dir` points to where you ran `mlflow server`. Look for `mlruns/` and `mlartifacts/` folders.\n",
        "\n",
        "### Problem: \"Permission denied\" on remote server\n",
        "**Solution**: Verify your username has write access. Contact server administrator if needed.\n",
        "\n",
        "### Problem: Missing preprocessing code on remote\n",
        "**Solution**: Make sure you logged the preprocessing directory with `mlflow.log_artifacts()`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
