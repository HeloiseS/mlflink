{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial 1: MLflow Local Setup and First Run\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "- Understand how to set up a local MLflow tracking server\n",
        "- Learn to log parameters, metrics, models, and artifacts\n",
        "- Explore the MLflow UI to compare experiments\n",
        "- Understand best practices for local experimentation\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "- Python ‚â• 3.9\n",
        "- Basic knowledge of machine learning and scikit-learn\n",
        "- Familiarity with pandas DataFrames\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n",
        "\n",
        "First, ensure all dependencies are installed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you haven't installed the package yet, run:\n",
        "# pip install -e .[dev]\n",
        "\n",
        "# Verify MLflow is installed\n",
        "import mlflow\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Starting Your Local MLflow Server\n",
        "\n",
        "### üöÄ Launch the server\n",
        "\n",
        "Open a **new terminal** window and run:\n",
        "\n",
        "```bash\n",
        "mlflow server --host 127.0.0.1 --port 6969\n",
        "```\n",
        "\n",
        "**Important Notes:**\n",
        "- Keep this terminal running during your experiments\n",
        "- You can choose any available port (6969 is just an example)\n",
        "- The server will create `mlruns/` and `mlartifacts/` directories in your current location\n",
        "\n",
        "### üì∏ Screenshot placeholder: MLflow server running in terminal\n",
        "![image.png](assets/mlfflow_server_run__on_terminal.png)\n",
        "\n",
        "---\n",
        "\n",
        "### üåê Access the UI\n",
        "\n",
        "Open your browser and navigate to:\n",
        "```\n",
        "http://127.0.0.1:6969\n",
        "```\n",
        "\n",
        "You should see the MLflow UI (initially empty).\n",
        "\n",
        "### üì∏ Screenshot placeholder: MLflow UI homepage\n",
        "![image.png](assets/Empty%20MLflow%20UI%20with%20no%20experiments%20yet.png)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure MLflow Connection\n",
        "\n",
        "Now let's connect our Python code to the local MLflow server:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Configuration\n",
        "HOST = \"http://127.0.0.1\"  # Your local host\n",
        "PORT = 6969                # Must match the port you used in the terminal\n",
        "\n",
        "# Set the tracking URI (where MLflow will log everything)\n",
        "mlflow.set_tracking_uri(f\"{HOST}:{PORT}\")\n",
        "\n",
        "# Create or set an experiment\n",
        "# Experiments help organize related runs\n",
        "EXPERIMENT = \"tutorial\"\n",
        "mlflow.set_experiment(EXPERIMENT)\n",
        "\n",
        "# Instantiate the MLflow client (useful for advanced operations)\n",
        "client = MlflowClient()\n",
        "\n",
        "print(f\"‚úÖ Connected to MLflow at {HOST}:{PORT}\")\n",
        "print(f\"‚úÖ Experiment set to: {EXPERIMENT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Key Concepts\n",
        "\n",
        "- **Tracking URI**: Where MLflow stores your experiments (can be local or remote)\n",
        "- **Experiment**: A collection of related runs (e.g., all runs for a specific project)\n",
        "- **Run**: A single execution of your training code with specific parameters\n",
        "\n",
        "\n",
        "\n",
        "![tutorial_experiement.png](assets/tutorial_experiement.png)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Understanding the Data Pipeline\n",
        "\n",
        "Before training, let's understand the preprocessing pipeline. This project uses a structured approach:\n",
        "\n",
        "```\n",
        "Raw Alerts ‚Üí Cut Alerts ‚Üí Clean Data ‚Üí Curated Data ‚Üí Features (X matrix)\n",
        "```\n",
        "\n",
        "Let's load some data and process it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.abspath(\"..\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflink.processing import preprocessing as pp\n",
        "from importlib import resources\n",
        "\n",
        "# Get data from local file, live stream or mocked data stream\n",
        "with resources.path(\"mlflink.data\", \"test_alerts.parquet\") as parquet_path:\n",
        "    PARQUET_FILE = parquet_path\n",
        "\n",
        "alerts_df = pd.read_parquet(PARQUET_FILE)\n",
        "print(f\"üìä Loaded {len(alerts_df)} alerts\")\n",
        "print(f\"üìã Columns: {alerts_df.columns.tolist()[:5]}...\")  # Show first 5 columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Apply cuts to filter relevant alerts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply quality cuts (defined in preprocessing.py)\n",
        "# This filters alerts based on criteria like magnitude, classification, etc.\n",
        "cut_alerts_df = pp.make_cut(alerts_df)\n",
        "print(f\"  After cuts: {len(cut_alerts_df)} alerts remaining\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Extract and clean relevant columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract only the columns we need for training\n",
        "clean_df = pp.raw2clean(cut_alerts_df)\n",
        "print(f\"üßπ Clean dataframe shape: {clean_df.shape}\")\n",
        "print(f\"üìã Clean columns: {clean_df.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Optional curation (e.g., cross-matching)\n",
        "\n",
        "‚ö†Ô∏è **Note**: `run_sherlock()` requires a LASAIR_TOKEN environment variable. If not set, it will skip this step gracefully.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This step adds additional information via cross-matching services\n",
        "# For this tutorial, it will skip if LASAIR_TOKEN is not set\n",
        "curated_df = pp.run_sherlock(clean_df)\n",
        "print(f\" Curated dataframe shape: {curated_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Create features matrix (X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform curated data into ML-ready features\n",
        "X, ids = pp.make_X(curated_df)\n",
        "\n",
        "print(f\" Features matrix shape: {X.shape}\")\n",
        "print(f\" Feature columns: {X.columns.tolist()}\")\n",
        "print(f\"\\nFirst few features:\")\n",
        "display(X.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create mock labels for demonstration\n",
        "\n",
        "In a real scenario, you would have actual labels. For this tutorial, we'll create dummy labels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fake labels (in real use, you'd have actual labels)\n",
        "y = np.array([0] * X.shape[0])\n",
        "y = pd.DataFrame(y, columns=[\"labels\"])\n",
        "\n",
        "print(f\" Labels shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Your First MLflow Run\n",
        "\n",
        "Now comes the exciting part! We'll train a model and log everything with MLflow.\n",
        "\n",
        "### What we'll log:\n",
        "1.  **Parameters** (hyperparameters)\n",
        "2.  **Metrics** (accuracy, precision, recall, F1)\n",
        "3.  **Model** (the trained model)\n",
        "4.  **Data** (training data - optional, see warning below)\n",
        "5.  **Artifacts** (metadata, custom files)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define model hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from mlflow.models import infer_signature \n",
        "import json\n",
        "\n",
        "# Define hyperparameters\n",
        "# üí° TIP: Use descriptive parameter names that match your model's API\n",
        "PARAMS = {\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"random_state\": 42\n",
        "}\n",
        "\n",
        "print(\" Hyperparameters:\")\n",
        "for key, value in PARAMS.items():\n",
        "    print(f\"  - {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start MLflow run and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start an MLflow run\n",
        "# üí° TIP: Give runs descriptive names so you can identify them later\n",
        "with mlflow.start_run(run_name=f\"test_LR_{PARAMS['learning_rate']}\"):\n",
        "    \n",
        "    print(\"üöÄ MLflow run started!\\n\")\n",
        "    \n",
        "    # ==========================================\n",
        "    # 1. TRAIN YOUR MODEL (as you normally would)\n",
        "    # ==========================================\n",
        "    print(\"üèãÔ∏è  Training model...\")\n",
        "    model = HistGradientBoostingClassifier(**PARAMS)\n",
        "    model.fit(X.values, y)\n",
        "    y_pred = model.predict(X.values)\n",
        "    print(\"‚úÖ Model trained!\\n\")\n",
        "    \n",
        "    # ==========================================\n",
        "    # 2. LOG PARAMETERS\n",
        "    # ==========================================\n",
        "    print(\"üìù Logging parameters...\")\n",
        "    mlflow.log_params(PARAMS)\n",
        "    \n",
        "    # ==========================================\n",
        "    # 3. LOG THE MODEL\n",
        "    # ==========================================\n",
        "    print(\"üíæ Logging model...\")\n",
        "    signature = infer_signature(X, y_pred) # Define model input/output schema\n",
        "    mlflow.sklearn.log_model(\n",
        "        model,\n",
        "        name=\"model_name\",  # Where it's stored in MLflow\n",
        "        signature=signature, \n",
        "        input_example=X.iloc[:1],  # Example input for documentation\n",
        "    )\n",
        "    \n",
        "    # ==========================================\n",
        "    # 4. LOG METRICS\n",
        "    # ==========================================\n",
        "    print(\"üìä Calculating and logging metrics...\")\n",
        "    \n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    mlflow.log_metric(\"accuracy\", acc)\n",
        "    print(f\"  - Accuracy: {acc:.4f}\")\n",
        "    \n",
        "    prec = precision_score(y, y_pred, zero_division=0)\n",
        "    mlflow.log_metric(\"precision\", prec)\n",
        "    print(f\"  - Precision: {prec:.4f}\")\n",
        "    \n",
        "    recall = recall_score(y, y_pred, zero_division=0)\n",
        "    mlflow.log_metric(\"recall\", recall)\n",
        "    print(f\"  - Recall: {recall:.4f}\")\n",
        "    \n",
        "    f1 = f1_score(y, y_pred, zero_division=0)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "    print(f\"  - F1-score: {f1:.4f}\\n\")\n",
        "    \n",
        "    # ==========================================\n",
        "    # 5. LOG DATA (OPTIONAL - READ WARNING BELOW)\n",
        "    # ==========================================\n",
        "    print(\"üíø Logging training data...\")\n",
        "    mlflow.log_table(X, \"X_train.parquet\")\n",
        "    mlflow.log_table(y, \"y_train.parquet\")\n",
        "    \n",
        "    # ==========================================\n",
        "    # 6. LOG ADDITIONAL METADATA\n",
        "    # ==========================================\n",
        "    print(\"üìã Logging metadata...\")\n",
        "    meta_info = {\n",
        "        \"params\": PARAMS,\n",
        "        \"data_info\": {\n",
        "            \"n_samples\": X.shape[0],\n",
        "            \"n_features\": X.shape[1]\n",
        "        },\n",
        "        \"notes\": \"First tutorial run with local MLflow\"\n",
        "    }\n",
        "    \n",
        "    with open(\"meta.json\", \"w\") as f:\n",
        "        json.dump(meta_info, f, indent=2)\n",
        "    mlflow.log_artifact(\"meta.json\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Run completed successfully!\")\n",
        "    print(f\"üîó View in UI: {HOST}:{PORT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚ö†Ô∏è Important Warning: Logging Large Datasets\n",
        "\n",
        "```python\n",
        "mlflow.log_table(X, \"X_train.parquet\")\n",
        "```\n",
        "\n",
        "**When to log data:**\n",
        "- ‚úÖ For final, production-ready models\n",
        "- ‚úÖ When you need complete reproducibility\n",
        "- ‚úÖ For small-to-medium datasets (< 100 MB)\n",
        "\n",
        "**When NOT to log data:**\n",
        "- ‚ùå During frequent experimentation\n",
        "- ‚ùå With large datasets (> 1 GB)\n",
        "- ‚ùå When sending to remote servers (avoid overload)\n",
        "\n",
        "**üí° Best Practice**: \n",
        "- Experiment locally WITHOUT logging data\n",
        "- Once you have a winning model, do ONE final run WITH data logging\n",
        "- Only send that final run to remote servers\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exploring the MLflow UI\n",
        "\n",
        "Now that we've logged a run, let's explore what MLflow captured!\n",
        "\n",
        "### üåê Open the UI\n",
        "\n",
        "Go to your browser: `http://127.0.0.1:6969`\n",
        "\n",
        "You should see:\n",
        "\n",
        "### üì∏ Screenshot placeholder: Experiment view\n",
        "![tutorial_experiement.png](assets/tutorial_experiement.png)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Click on your run\n",
        "\n",
        "You'll see detailed information:\n",
        "\n",
        "1. **Parameters tab**: Your hyperparameters\n",
        "2. **Metrics tab**: Accuracy, precision, recall, F1\n",
        "3. **Artifacts tab**: Model, data files, metadata\n",
        "\n",
        "### üì∏ Screenshot placeholder: Run details\n",
        "![first_run.png](assets/first_run.png)\n",
        "\n",
        "![image.png](assets/overview%20details.png)\n",
        "\n",
        "![artifact.png](assets/artifact.png)\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Explore the Model\n",
        "\n",
        "Click on **Artifacts** ‚Üí **model**\n",
        "\n",
        "You'll see:\n",
        "- `MLmodel` file (metadata)\n",
        "- `model.pkl` (your actual model)\n",
        "- `conda.yaml` (environment info)\n",
        "- `requirements.txt` (dependencies)\n",
        "\n",
        "### üì∏ Screenshot placeholder: Model artifacts\n",
        "![image.png](assets/click%20on%20model.png)\n",
        "\n",
        "![image.png](assets/model_artifact.png)\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Multiple Experiments\n",
        "\n",
        "The real power of MLflow comes from comparing multiple runs. Let's train with different learning rates:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different learning rates\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
        "\n",
        "print(\" üèÑ Running experiments with different learning rates...\\n\")\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"üèÉ‚Äç‚ôÄÔ∏è Running with learning_rate={lr}\")\n",
        "    \n",
        "    with mlflow.start_run(run_name=f\"lr_{lr}\"):\n",
        "        # Update parameters\n",
        "        params = {\"learning_rate\": lr, \"random_state\": 42}\n",
        "        \n",
        "        # Train\n",
        "        model = HistGradientBoostingClassifier(**params)\n",
        "        model.fit(X.values, y)\n",
        "        y_pred = model.predict(X.values)\n",
        "        \n",
        "        # Log\n",
        "        mlflow.log_params(params)\n",
        "        mlflow.log_metric(\"accuracy\", accuracy_score(y, y_pred))\n",
        "        mlflow.log_metric(\"f1_score\", f1_score(y, y_pred, zero_division=0))\n",
        "        \n",
        "        signature = infer_signature(X, y_pred)\n",
        "        mlflow.sklearn.log_model(\n",
        "            model,\n",
        "            artifact_path=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=X.iloc[:1]\n",
        "        )\n",
        "\n",
        "    print(f\"Completed\\n\")\n",
        "\n",
        "print(\"üéâ All experiments completed!\")\n",
        "print(f\"\\nüîó Compare runs in UI: {HOST}:{PORT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Compare Runs in the UI\n",
        "\n",
        "1. Go to the MLflow UI\n",
        "2. Select multiple runs (checkboxes)\n",
        "3. Click **Compare**\n",
        "4. View side-by-side parameter and metric comparisons\n",
        "\n",
        "### üì∏ Screenshot placeholder: Run comparison\n",
        "![compare_run.png](assets/compare_run.png)\n",
        "\n",
        "![compare_runs_in experiment.png](<assets/compare_runs_in experiment.png>)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Programmatically Query Runs\n",
        "\n",
        "You can also access run data programmatically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all runs from the current experiment\n",
        "experiment = client.get_experiment_by_name(EXPERIMENT)\n",
        "runs = client.search_runs(experiment.experiment_id)\n",
        "\n",
        "print(f\"üìä Found {len(runs)} runs in experiment '{EXPERIMENT}'\\n\")\n",
        "\n",
        "# Display run information\n",
        "run_data = []\n",
        "for run in runs:\n",
        "    run_data.append({\n",
        "        \"Run Name\": run.data.tags.get(\"mlflow.runName\", \"N/A\"),\n",
        "        \"Learning Rate\": run.data.params.get(\"learning_rate\", \"N/A\"),\n",
        "        \"Accuracy\": run.data.metrics.get(\"accuracy\", \"N/A\"),\n",
        "        \"F1 Score\": run.data.metrics.get(\"f1_score\", \"N/A\"),\n",
        "        \"Run ID\": run.info.run_id[:8] + \"...\"  # Shortened for display\n",
        "    })\n",
        "\n",
        "runs_df = pd.DataFrame(run_data)\n",
        "display(runs_df.sort_values(\"Accuracy\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Best Practices Summary\n",
        "\n",
        "### ‚úÖ DO:\n",
        "- Give runs descriptive names (e.g., `lr_0.1_dropout_0.3`)\n",
        "- Log all hyperparameters consistently\n",
        "- Log multiple metrics (not just accuracy)\n",
        "- Use experiments to organize related work\n",
        "- Add metadata/notes for context\n",
        "\n",
        "### üôÖüèø DON'T:\n",
        "- Log large datasets on every run\n",
        "- Forget to log important parameters\n",
        "- Mix unrelated experiments in the same experiment name\n",
        "- Delete `mlruns/` and `mlartifacts/` directories (unless you want to lose everything!)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Where to Find Your Data\n",
        "\n",
        "MLflow stores everything locally in:\n",
        "\n",
        "```\n",
        "your_working_directory/\n",
        "‚îú‚îÄ‚îÄ mlruns/              # Metadata, parameters, metrics\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ <experiment_id>/\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ <run_id>/\n",
        "‚îÇ           ‚îî‚îÄ‚îÄ meta.yaml\n",
        "‚îî‚îÄ‚îÄ mlartifacts/         # Models, data, artifacts\n",
        "    ‚îî‚îÄ‚îÄ <experiment_id>/\n",
        "        ‚îî‚îÄ‚îÄ <run_id>/\n",
        "            ‚îú‚îÄ‚îÄ artifacts/\n",
        "            ‚îÇ   ‚îú‚îÄ‚îÄ model/\n",
        "            ‚îÇ   ‚îú‚îÄ‚îÄ X_train.parquet\n",
        "            ‚îÇ   ‚îî‚îÄ‚îÄ meta.json\n",
        "```\n",
        "\n",
        "üí° **Tip**: You'll need these paths in Tutorial 2 when sending runs to a remote server!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Summary\n",
        "\n",
        "Congratulations! You've learned:\n",
        "\n",
        "‚úÖ How to set up a local MLflow tracking server  \n",
        "‚úÖ How to log parameters, metrics, models, and artifacts  \n",
        "‚úÖ How to navigate the MLflow UI  \n",
        "‚úÖ How to compare multiple runs  \n",
        "‚úÖ Best practices for local experimentation  \n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "In **Tutorial 2**, you'll learn how to:\n",
        "- Select your best runs locally\n",
        "- Send **only** the successful runs to a remote MLflow server\n",
        "- Avoid overloading remote servers with unnecessary data\n",
        "\n",
        "---\n",
        "\n",
        "## üÜò Troubleshooting\n",
        "\n",
        "### Problem: \"Connection refused\" error\n",
        "**Solution**: Make sure the MLflow server is running in a separate terminal.\n",
        "\n",
        "### Problem: Can't find runs in UI\n",
        "**Solution**: Check you're using the correct port and that the server is running from the same directory.\n",
        "\n",
        "<!-- ### Problem: \"Module not found\" errors\n",
        "**Solution**: Install the package: `pip install -e .[dev]` -->\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
